{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To mount the drive with colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bnlp_toolkit # package for bangla text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"অতএব\",\"অথচ\",\"অথবা\",\"অনুযায়ী\",\"অনেক\",\"অনেকে\",\"অনেকেই\",\"অন্তত\",\"অন্য\",\"অবধি\",\"অবশ্য\",\"অর্থাত\",\"আই\",\"আগামী\",\"আগে\",\"আগেই\",\"আছে\",\"আজ\",\"আদ্যভাগে\",\"আপনার\",\"আপনি\",\"আবার\",\"আমরা\",\"আমাকে\",\"আমাদের\",\"আমার\",\"আমি\",\"আর\",\"আরও\",\"ই\",\"ইত্যাদি\",\"ইহা\",\"উচিত\",\"উত্তর\",\"উনি\",\"উপর\",\"উপরে\",\"এ\",\"এঁদের\",\"এঁরা\",\"এই\",\"একই\",\"একটি\",\"একবার\",\"একে\",\"এক্\",\"এখন\",\"এখনও\",\"এখানে\",\"এখানেই\",\"এটা\",\"এটাই\",\"এটি\",\"এত\",\"এতটাই\",\"এতে\",\"এদের\",\"এব\",\"এবং\",\"এবার\",\"এমন\",\"এমনকী\",\"এমনি\",\"এর\",\"এরা\",\"এল\",\"এস\",\"এসে\",\"ঐ\",\"ও\",\"ওঁদের\",\"ওঁর\",\"ওঁরা\",\"ওই\",\"ওকে\",\"ওখানে\",\"ওদের\",\"ওর\",\"ওরা\",\"কখনও\",\"কত\",\"কবে\",\"কমনে\",\"কয়েক\",\"কয়েকটি\",\"করছে\",\"করছেন\",\"করতে\",\"করবে\",\"করবেন\",\"করলে\",\"করলেন\",\"করা\",\"করাই\",\"করায়\",\"করার\",\"করি\",\"করিতে\",\"করিয়া\",\"করিয়ে\",\"করে\",\"করেই\",\"করেছিলেন\",\"করেছে\",\"করেছেন\",\"করেন\",\"কাউকে\",\"কাছ\",\"কাছে\",\"কাজ\",\"কাজে\",\"কারও\",\"কারণ\",\"কি\",\"কিংবা\",\"কিছু\",\"কিছুই\",\"কিন্তু\",\"কী\",\"কে\",\"কেউ\",\"কেউই\",\"কেখা\",\"কেন\",\"কোটি\",\"কোন\",\"কোনও\",\"কোনো\",\"ক্ষেত্রে\",\"কয়েক\",\"খুব\",\"গিয়ে\",\"গিয়েছে\",\"গিয়ে\",\"গুলি\",\"গেছে\",\"গেল\",\"গেলে\",\"গোটা\",\"চলে\",\"চান\",\"চায়\",\"চার\",\"চালু\",\"চেয়ে\",\"চেষ্টা\",\"ছাড়া\",\"ছাড়াও\",\"ছিল\",\"ছিলেন\",\"জন\",\"জনকে\",\"জনের\",\"জন্য\",\"জন্যওজে\",\"জানতে\",\"জানা\",\"জানানো\",\"জানায়\",\"জানিয়ে\",\"জানিয়েছে\",\"জে\",\"জ্নজন\",\"টি\",\"ঠিক\",\"তখন\",\"তত\",\"তথা\",\"তবু\",\"তবে\",\"তা\",\"তাঁকে\",\"তাঁদের\",\"তাঁর\",\"তাঁরা\",\"তাঁাহারা\",\"তাই\",\"তাও\",\"তাকে\",\"তাতে\",\"তাদের\",\"তার\",\"তারপর\",\"তারা\",\"তারৈ\",\"তাহলে\",\"তাহা\",\"তাহাতে\",\"তাহার\",\"তিনঐ\",\"তিনি\",\"তিনিও\",\"তুমি\",\"তুলে\",\"তেমন\",\"তো\",\"তোমার\",\"থাকবে\",\"থাকবেন\",\"থাকা\",\"থাকায়\",\"থাকে\",\"থাকেন\",\"থেকে\",\"থেকেই\",\"থেকেও\",\"দিকে\",\"দিতে\",\"দিন\",\"দিয়ে\",\"দিয়েছে\",\"দিয়েছেন\",\"দিলেন\",\"দু\",\"দুই\",\"দুটি\",\"দুটো\",\"দেওয়া\",\"দেওয়ার\",\"দেওয়া\",\"দেখতে\",\"দেখা\",\"দেখে\",\"দেন\",\"দেয়\",\"দ্বারা\",\"ধরা\",\"ধরে\",\"ধামার\",\"নতুন\",\"নয়\",\"না\",\"নাই\",\"নাকি\",\"নাগাদ\",\"নানা\",\"নিজে\",\"নিজেই\",\"নিজেদের\",\"নিজের\",\"নিতে\",\"নিয়ে\",\"নিয়ে\",\"নেই\",\"নেওয়া\",\"নেওয়ার\",\"নেওয়া\",\"নয়\",\"পক্ষে\",\"পর\",\"পরে\",\"পরেই\",\"পরেও\",\"পর্যন্ত\",\"পাওয়া\",\"পাচ\",\"পারি\",\"পারে\",\"পারেন\",\"পি\",\"পেয়ে\",\"পেয়্র্\",\"প্রতি\",\"প্রথম\",\"প্রভৃতি\",\"প্রযন্ত\",\"প্রাথমিক\",\"প্রায়\",\"প্রায়\",\"ফলে\",\"ফিরে\",\"ফের\",\"বক্তব্য\",\"বদলে\",\"বন\",\"বরং\",\"বলতে\",\"বলল\",\"বললেন\",\"বলা\",\"বলে\",\"বলেছেন\",\"বলেন\",\"বসে\",\"বহু\",\"বা\",\"বাদে\",\"বার\",\"বি\",\"বিনা\",\"বিভিন্ন\",\"বিশেষ\",\"বিষয়টি\",\"বেশ\",\"বেশি\",\"ব্যবহার\",\"ব্যাপারে\",\"ভাবে\",\"ভাবেই\",\"মতো\",\"মতোই\",\"মধ্যভাগে\",\"মধ্যে\",\"মধ্যেই\",\"মধ্যেও\",\"মনে\",\"মাত্র\",\"মাধ্যমে\",\"মোট\",\"মোটেই\",\"যখন\",\"যত\",\"যতটা\",\"যথেষ্ট\",\"যদি\",\"যদিও\",\"যা\",\"যাঁর\",\"যাঁরা\",\"যাওয়া\",\"যাওয়ার\",\"যাওয়া\",\"যাকে\",\"যাচ্ছে\",\"যাতে\",\"যাদের\",\"যান\",\"যাবে\",\"যায়\",\"যার\",\"যারা\",\"যিনি\",\"যে\",\"যেখানে\",\"যেতে\",\"যেন\",\"যেমন\",\"র\",\"রকম\",\"রয়েছে\",\"রাখা\",\"রেখে\",\"লক্ষ\",\"শুধু\",\"শুরু\",\"সঙ্গে\",\"সঙ্গেও\",\"সব\",\"সবার\",\"সমস্ত\",\"সম্প্রতি\",\"সহ\",\"সহিত\",\"সাধারণ\",\"সামনে\",\"সি\",\"সুতরাং\",\"সে\",\"সেই\",\"সেখান\",\"সেখানে\",\"সেটা\",\"সেটাই\",\"সেটাও\",\"সেটি\",\"স্পষ্ট\",\"স্বয়ং\",\"হইতে\",\"হইবে\",\"হইয়া\",\"হওয়া\",\"হওয়ায়\",\"হওয়ার\",\"হচ্ছে\",\"হত\",\"হতে\",\"হতেই\",\"হন\",\"হবে\",\"হবেন\",\"হয়\",\"হয়তো\",\"হয়নি\",\"হয়ে\",\"হয়েই\",\"হয়েছিল\",\"হয়েছে\",\"হয়েছেন\",\"হল\",\"হলে\",\"হলেই\",\"হলেও\",\"হলো\",\"হাজার\",\"হিসাবে\",\"হৈলে\",\"হোক\",\"হয়\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re,json,nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "path ='/content/drive/MyDrive/cse440/'\n",
    "#stopwords_list = stopwords\n",
    "class color: # Text style\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mounting dataset into colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/cse440/Dataset_full.csv',encoding='UTF-8')\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total comments:\",len(data),\n",
    "      \"\\nTotal Positive comments:\",len(data[data.Tag =='Positive']),\n",
    "      \"\\nTotal Negative comments:\",len(data[data.Tag=='Negative']),\n",
    "      \"\\nTotal Neutral comments:\",len(data[data.Tag =='Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some unprocessed reviews\n",
    "sample_data = [10,1000,1500,2000,2500,6000,6500,8000,9001,10000,11000]\n",
    "for i in sample_data:\n",
    "      print(data.Comments[i],'\\n','Sentiment:-- ',data.Tag[i],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "data['Tag'].value_counts().plot(kind='barh', figsize=(9, 3))\n",
    "plt.xlabel(\"Number of Comments\", labelpad=12)\n",
    "plt.ylabel(\"Sentiment Class\", labelpad=12)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title(\"Dataset Distribution\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning function\n",
    "def process_comments(Comment):\n",
    "    Comment = re.sub('[^\\u0980-\\u09FF]',' ',str(Comment)) #removing unnecessary punctuation\n",
    "    return Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function into the dataframe\n",
    "data['cleaned'] = data['Comments'].apply(process_comments)\n",
    "\n",
    "# print some cleaned reviews from the dataset\n",
    "sample_data = [10,100,25,854,966,1500,1589,1700,2000,2500,4000,4500]\n",
    "for i in sample_data:\n",
    "     print('Original:\\n',data.Comments[i],'\\nCleaned:\\n',\n",
    "           data.cleaned[i],'\\n','Sentiment:-- ',data.Tag[i],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Low Length Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each Reveiws\n",
    "data['length'] = data['cleaned'].apply(lambda x:len(x.split()))\n",
    "# Remove the reviews with least words\n",
    "dataset = data.loc[data.length>1]\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "print(\"After Cleaning:\",\"\\nRemoved {} Small Reviews\".format(len(data)-len(dataset)),\n",
    "      \"\\nTotal Reviews:\",len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    This function will print the summary of the reviews and words distribution in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: list of cleaned sentences\n",
    "\n",
    "    Returns:\n",
    "        Number of documnets per class: int\n",
    "        Number of words per class: int\n",
    "        Number of unique words per class: int\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    words = []\n",
    "    u_words = []\n",
    "    total_u_words = [word.strip().lower() for t in list(dataset.cleaned) for word in t.strip().split()]\n",
    "    class_label= [k for k,v in dataset.Tag.value_counts().to_dict().items()]\n",
    "  # find word list\n",
    "    for label in class_label:\n",
    "        word_list = [word.strip().lower() for t in list(dataset[dataset.Tag==label].cleaned) for word in t.strip().split()]\n",
    "        counts = dict()\n",
    "        for word in word_list:\n",
    "                counts[word] = counts.get(word, 0)+1\n",
    "        # sort the dictionary of word list\n",
    "        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)\n",
    "        # Documents per class\n",
    "        documents.append(len(list(dataset[dataset.Tag==label].cleaned)))\n",
    "        # Total Word per class\n",
    "        words.append(len(word_list))\n",
    "        # Unique words per class\n",
    "        u_words.append(len(np.unique(word_list)))\n",
    "\n",
    "        print(\"\\nClass Name : \",label)\n",
    "        print(\"Number of Documents:{}\".format(len(list(dataset[dataset.Tag==label].cleaned))))\n",
    "        print(\"Number of Words:{}\".format(len(word_list)))\n",
    "        print(\"Number of Unique Words:{}\".format(len(np.unique(word_list))))\n",
    "        print(\"Most Frequent Words:\\n\")\n",
    "        for k,v in ordered[:10]:\n",
    "              print(\"{}\\t{}\".format(k,v))\n",
    "    print(\"Total Number of Unique Words:{}\".format(len(np.unique(total_u_words))))\n",
    "\n",
    "    return documents,words,u_words,class_label\n",
    "\n",
    "#call the fucntion\n",
    "documents,words,u_words,class_names = data_summary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = pd.DataFrame({'Total Documents':documents,\n",
    "                            'Total Words':words,\n",
    "                            'Unique Words':u_words,\n",
    "                            'Class Names':class_names})\n",
    "df = pd.melt(data_matrix, id_vars=\"Class Names\", var_name=\"Category\", value_name=\"Values\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot()\n",
    "\n",
    "sns.barplot(data=df,x='Class Names', y='Values' ,hue='Category')\n",
    "ax.set_xlabel('Class Names')\n",
    "ax.set_title('Data Statistics')\n",
    "\n",
    "ax.xaxis.set_ticklabels(class_names, rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Review of each of the Review\n",
    "dataset['ReviewLength'] = dataset.cleaned.apply(lambda x:len(x.split()))\n",
    "frequency = dict()\n",
    "for i in dataset.ReviewLength:\n",
    "    frequency[i] = frequency.get(i, 0)+1\n",
    "\n",
    "plt.bar(frequency.keys(), frequency.values(), color =\"b\")\n",
    "plt.xlim(1, 135)\n",
    "\n",
    "plt.xlabel('Lenght of the Texts')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Length-Frequency Distribution')\n",
    "plt.show()\n",
    "print(f\"Maximum Length of a review: {max(dataset.ReviewLength)}\")\n",
    "print(f\"Minimum Length of a review: {min(dataset.ReviewLength)}\")\n",
    "print(f\"Average Length of a reviews: {round(np.mean(dataset.ReviewLength),0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding and Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = dataset[dataset.Tag == 'Positive'] - Samaira\n",
    "negative = dataset[dataset.Tag == 'Negative']\n",
    "\n",
    "dataset = pd.concat([positive, negative],ignore_index=True)\n",
    "\n",
    "dataset.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "                                       ################# Label Encoding Function #########\n",
    "                                       #==================================================\n",
    "\n",
    "def label_encoding(sentiment,bool):\n",
    "    \"\"\"\n",
    "    This function will return the encoded labels in array format.\n",
    "\n",
    "    Args:\n",
    "        sentiment: series of class names(str)\n",
    "        bool: boolean (True or False)\n",
    "\n",
    "    Returns:\n",
    "        labels: numpy array\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sentiment)\n",
    "    encoded_labels = le.transform(sentiment)\n",
    "    labels = np.array(encoded_labels) # Converting into numpy array\n",
    "    class_names =le.classes_ ## Define the class names again\n",
    "    if bool == True:\n",
    "        print(\"\\n\\t\\t\\t===== Label Encoding =====\",\"\\nClass Names:-->\",le.classes_)\n",
    "        for i in sample_data:\n",
    "            print(sentiment[i],' ', encoded_labels[i],'\\n')\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.cleaned = dataset.cleaned.apply(lambda x:x.replace('\\n',' ')) - samaira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================\n",
    "                                            ##### Unigram Tf-idf value calculation - samaira\n",
    "                                            #======================================\n",
    "\n",
    "def calc_gram_tfidf(reviews,gram):\n",
    "    \"\"\"\n",
    "    This function will return the tf-idf value of the respective gram features .\n",
    "\n",
    "    Args:\n",
    "        reviews: a list of cleaned reviews\n",
    "\n",
    "    Returns:\n",
    "        tfidf: a instance of TfidfVectorizer\n",
    "        X : Unigram Feature Vector (sparse matrix)\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(ngram_range=gram,use_idf=True,tokenizer=lambda x: x.split())\n",
    "    X = tfidf.fit_transform(reviews)\n",
    "\n",
    "    return tfidf,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression - samaira\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performane into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "                                        #### Model Performane into Dataframe #####\n",
    "                                        #=========================================\n",
    "\n",
    "def performance_table(performance_dict):\n",
    "    \"\"\"\n",
    "    This function will create a dataframe of all the performance parameters.\n",
    "\n",
    "    Args:\n",
    "        performance_dict: a dictionary of all the parameters for each models\n",
    "\n",
    "    Returns:\n",
    "        performance_df: a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    acc_list = []\n",
    "    pr_list = []\n",
    "    re_list = []\n",
    "    f1_list = []\n",
    "    for i in performance_dict.keys():\n",
    "        acc_list.append(performance_dict[i]['Accuracy'])\n",
    "        pr_list.append(performance_dict[i]['Precision'])\n",
    "        re_list.append(performance_dict[i]['Recall'])\n",
    "        f1_list.append(performance_dict[i]['F1 Score'])\n",
    "\n",
    "\n",
    "    # Create a dataframe\n",
    "    model_names = ['LR','DT','RF','MNB','KNN','Linear SVM','RBF SVM']\n",
    "    performance_df = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n",
    "                                   'Recall':re_list,'F1 Score':f1_list,\n",
    "                                   'Model Name':model_names })\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the json files\n",
    "gram_names = ['Unigram','Bigram','Trigram']\n",
    "\n",
    "for gram in gram_names:\n",
    "  accuracy = json.load(open(path+f'ml_performance_{gram}.json'))\n",
    "  table = performance_table(accuracy)\n",
    "  print(f\"\\n========== Performace Table for {gram} feature:========\\n\",table)\n",
    "  print(f\"\\n========In case of {gram} feature:========\\n\")\n",
    "  print(f\"Highest Accuracy achieved by {table['Model Name'][table.Accuracy.idxmax(axis = 0)]} at = {max(table.Accuracy)}\")\n",
    "  print(f\"Highest F1-Score achieved by {table['Model Name'][table['F1 Score'].idxmax(axis = 0)]} at = {max(table['F1 Score'] )}\")\n",
    "  print(f\"Highest Precision Score achieved by {table['Model Name'][table['Precision'].idxmax(axis = 0)]} at = {max(table['Precision'] )}\")\n",
    "  print(f\"Highest Recall Score achieved by {table['Model Name'][table['Recall'].idxmax(axis = 0)]} at = {max(table['Recall'] )}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TRIGRAM, KNN had the best result. We apply cross val for KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric = 'minkowski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = knn_model.predict(X_test)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(knn_model, X_train, y_train, cv = 10, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #============================================\n",
    "                                           ####### Model Evaluation Function ############ - Arifur\n",
    "                                           #=============================================\n",
    "\n",
    "def model_performace(model,X_train,X_test,y_train,y_test):\n",
    "    \"\"\"\n",
    "    This function will return the performance parameter values of each ML models.\n",
    "    Performance parameters are Accuracy, F1-Score, Precision, Recall.\n",
    "\n",
    "    Args:\n",
    "        model: a ML model instance\n",
    "        X_train: training feature vector (sparse matrix)\n",
    "        X_test : testing feature vector (sparse matrix)\n",
    "        y_train: training encoded labels (array)\n",
    "        y_test : testing encoded labels (array)\n",
    "\n",
    "    Returns:\n",
    "        my_dict: a dictionary of all the parameters for each models\n",
    "    \"\"\"\n",
    "    my_dict = {}\n",
    "    model.fit(X_train,y_train)\n",
    "    # Prediction\n",
    "    pred_y = model.predict(X_test)\n",
    "\n",
    "    my_dict['Accuracy'] = round(accuracy_score(y_test, pred_y),4)*100\n",
    "    my_dict['Precision'] = round(precision_score(y_test, pred_y,average='weighted'),4)*100\n",
    "\n",
    "    my_dict['Recall'] = round(recall_score(y_test, pred_y,average='weighted'),4)*100\n",
    "    my_dict['F1 Score'] = round(f1_score(y_test, pred_y,average='weighted'),4)*100\n",
    "\n",
    "    return my_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_names = ['Unigram','Bigram','Trigram']\n",
    "\n",
    "for gram in gram_names:\n",
    "  accuracy = json.load(open(path+f'ml_performance_{gram}.json'))\n",
    "  table = performance_table(accuracy)\n",
    "  df = pd.melt(table[['Accuracy','F1 Score','Model Name']], id_vars=\"Model Name\", var_name=\"Category\", value_name=\"Values\")\n",
    "  plt.figure(figsize=(8,6))\n",
    "  ax = plt.subplot()\n",
    "  sns.barplot(data=df,x='Model Name', y='Values' ,hue='Category')\n",
    "  ax.set_xlabel('Model Name')\n",
    "  ax.set_title(f'Comparison of Accuracy and F1-Score Value for {gram} Feature')\n",
    "  ax.set_ylim([10,100])\n",
    "  ax.xaxis.set_ticklabels(table['Model Name'], rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **For BIGRAM, MNB had the best result. We apply cross val for MNB model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf,feature = calc_gram_tfidf(dataset.cleaned,(1,2))\n",
    "labels = label_encoding(dataset.Tag,False)\n",
    "X2_train,X2_test,y2_train,y2_test = dataset_split(feature,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = mnb_model.predict(X1_test)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(mnb_model, X1_train, y1_train, cv = 10, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
